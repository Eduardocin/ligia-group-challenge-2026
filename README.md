# Ligia Group Challenge 2026

<a target="_blank" href="https://cookiecutter-data-science.drivendata.org/">
    <img src="https://img.shields.io/badge/CCDS-Project%20template-328F97?logo=cookiecutter" />
</a>

Projeto de Data Science para an√°lise e predi√ß√£o de doen√ßas cardiovasculares

## üìä Dataset

Este projeto utiliza o **Cardiovascular Disease Dataset** do Kaggle, que cont√©m 1000 registros de pacientes com 14 vari√°veis relacionadas a fatores de risco cardiovascular.

### Vari√°veis do Dataset:
- **patientid**: ID do paciente
- **age**: Idade
- **gender**: G√™nero (0=Feminino, 1=Masculino)
- **chestpain**: Tipo de dor no peito
- **restingBP**: Press√£o arterial em repouso
- **serumcholestrol**: Colesterol s√©rico
- **fastingbloodsugar**: Glicemia em jejum
- **restingrelectro**: Resultados do eletrocardiograma em repouso
- **maxheartrate**: Frequ√™ncia card√≠aca m√°xima
- **exerciseangia**: Angina induzida por exerc√≠cio
- **oldpeak**: Depress√£o do segmento ST
- **slope**: Inclina√ß√£o do segmento ST
- **noofmajorvessels**: N√∫mero de vasos principais
- **target**: Presen√ßa de doen√ßa card√≠aca (0=N√£o, 1=Sim)

## üöÄ Setup do Ambiente

### Pr√©-requisitos
- [Miniconda](https://docs.conda.io/en/latest/miniconda.html) ou Anaconda instalado
- Git

### Instala√ß√£o

1. **Clone o reposit√≥rio**
   ```bash
   git clone <url-do-repositorio>
   cd ligia-group-challenge-2026
   ```

2. **Crie o ambiente conda**
   ```bash
   conda create -n ligia python=3.12 -y
   ```

3. **Ative o ambiente**
   ```bash
   conda activate ligia
   ```

4. **Instale as depend√™ncias**
   ```bash
   pip install -r requirements.txt
   ```

5. **Configure o kernel do Jupyter** (opcional, para usar notebooks)
   ```bash
   python -m ipykernel install --user --name=ligia --display-name="Python (ligia)"
   ```

### Verifica√ß√£o da Instala√ß√£o

```bash
# Verifique se o ambiente est√° ativo
conda info --envs

# Teste a instala√ß√£o de pacotes
python -c "import numpy, pandas, sklearn, mlcroissant; print('‚úÖ Ambiente configurado!')"
```

## üîÑ Fluxo de Execu√ß√£o

### ‚öôÔ∏è Requisitos para Automa√ß√£o com Make

O **Makefile** facilita a execu√ß√£o do projeto, mas requer configura√ß√£o espec√≠fica por plataforma:

<details>
<summary><b>üêß Linux/Mac</b></summary>

‚úÖ **Make j√° vem instalado!** Pode usar todos os comandos diretamente.

```bash
make pipeline
make app
```
</details>

<details>
<summary><b>ü™ü Windows - Op√ß√µes Dispon√≠veis</b></summary>

O Make **N√ÉO funciona nativamente** no Windows. Escolha uma das op√ß√µes:

#### **Op√ß√£o 1: WSL (Windows Subsystem for Linux) - Recomendado** ‚≠ê
```powershell
# 1. Instalar WSL (Execute como Administrador no PowerShell)
wsl --install

# 2. Reinicie o computador

# 3. Abra o WSL e navegue at√© o projeto
cd /mnt/c/Users/seu_usuario/caminho/do/projeto

# 4. Use comandos make normalmente
make pipeline
```

#### **Op√ß√£o 2: Git Bash** üîß
```bash
# Vem instalado com Git for Windows
# Abra Git Bash e execute:
make pipeline
make app
```
**Nota**: Alguns comandos podem n√£o funcionar 100% no Git Bash.

#### **Op√ß√£o 3: Sem Make - Comandos Python Diretos** üíª
```powershell
# Ative o ambiente primeiro
conda activate ligia

# Execute os scripts Python diretamente
python src/data_loader.py
python src/preprocessing.py
python src/model_training.py
streamlit run src/app.py
```
</details>

---

### üìã Op√ß√µes de Execu√ß√£o

#### **Op√ß√£o 1: Pipeline Completo Automatizado (Linux/Mac/WSL)**

```bash
make pipeline
```

Este comando executa automaticamente:
1. **Download dos dados** (`make download_data`) - Baixa o dataset do Kaggle via ML Croissant
2. **Pr√©-processamento** (`make preprocess`) - Limpa dados e cria features
3. **Treinamento** (`make train`) - Treina e salva o modelo

#### **Op√ß√£o 2: Executar Etapas Individualmente (Linux/Mac/WSL)**

```bash
# 1. Baixar dados
make download_data

# 2. Pr√©-processar dados
make preprocess

# 3. Treinar modelo
make train

# 4. Executar app
make app
```

#### **Op√ß√£o 3: Scripts Python Diretos (Todas as Plataformas)** ‚úÖ

```bash
# Certifique-se de ativar o ambiente primeiro
conda activate ligia

# 1. Baixar dados
python src/data_loader.py

# 2. Pr√©-processar dados
python src/preprocessing.py

# 3. Treinar modelo
python src/model_training.py

# 4. Executar app
streamlit run src/app.py
```

## üìä Como Executar o App Streamlit

### Linux/Mac/WSL
```bash
make app
```

### Todas as Plataformas (M√©todo Alternativo)
```bash
conda activate ligia
streamlit run src/app.py
```

Abra o navegador em `http://localhost:8501` para visualizar o app.



## üìÅ Project Organization

```
‚îú‚îÄ‚îÄ LICENSE                    <- Licen√ßa open-source do projeto
‚îú‚îÄ‚îÄ Makefile                   <- Automa√ß√£o com comandos: make pipeline, make app, etc.
‚îú‚îÄ‚îÄ README.md                  <- Documenta√ß√£o principal do projeto
‚îú‚îÄ‚îÄ pyproject.toml             <- Configura√ß√£o do projeto e metadados do pacote
‚îú‚îÄ‚îÄ requirements.txt           <- Depend√™ncias Python (gerado com pip freeze)
‚îÇ
‚îú‚îÄ‚îÄ data/                      <- Dados do projeto (n√£o versionados no Git)
‚îÇ   ‚îú‚îÄ‚îÄ external/              <- Dados de fontes externas
‚îÇ   ‚îú‚îÄ‚îÄ interim/               <- Dados intermedi√°rios transformados
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Cardiovascular_Disease_Dataset_Clean.csv
‚îÇ   ‚îú‚îÄ‚îÄ processed/             <- Datasets finais para modelagem
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ X_train.csv        <- Features de treino
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ X_test.csv         <- Features de teste
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ y_train.csv        <- Target de treino
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ y_test.csv         <- Target de teste
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scaler.pkl         <- Scaler treinado
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ selected_features.pkl <- Features selecionadas
‚îÇ   ‚îî‚îÄ‚îÄ raw/                   <- Dados originais imut√°veis
‚îÇ       ‚îî‚îÄ‚îÄ Cardiovascular_Disease_Dataset.csv
‚îÇ
‚îú‚îÄ‚îÄ dados_exames/              <- Dados de exames m√©dicos (PDFs)
‚îÇ
‚îú‚îÄ‚îÄ docs/                      <- Documenta√ß√£o do projeto
‚îÇ
‚îú‚îÄ‚îÄ models/                    <- Modelos treinados e serializados (.pkl)
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                 <- Jupyter notebooks para an√°lise explorat√≥ria
‚îÇ   ‚îú‚îÄ‚îÄ 1.0_carregamento_dados.ipynb      <- Download e carregamento inicial
‚îÇ   ‚îú‚îÄ‚îÄ 1.1_verificacao_qualidade.ipynb   <- Verifica√ß√£o de qualidade dos dados
‚îÇ   ‚îú‚îÄ‚îÄ 1.2_analise_univariada.ipynb      <- An√°lise de vari√°veis individuais
‚îÇ   ‚îú‚îÄ‚îÄ 1.3_analise_bivariada.ipynb       <- An√°lise de rela√ß√µes entre vari√°veis
‚îÇ   ‚îú‚îÄ‚îÄ 2.0_limpeza_dados.ipynb           <- Limpeza e tratamento de dados
‚îÇ   ‚îú‚îÄ‚îÄ 2.1_feature_engineering.ipynb     <- Cria√ß√£o de features
‚îÇ   ‚îú‚îÄ‚îÄ 3.1_treinamento_do_modelo.ipynb   <- Treinamento de modelos
‚îÇ   ‚îî‚îÄ‚îÄ 3.2_compara√ß√£o_com_baseline.ipynb <- Compara√ß√£o de modelos
‚îÇ
‚îú‚îÄ‚îÄ references/                <- Dicion√°rios de dados, manuais e materiais explicativos
‚îÇ
‚îú‚îÄ‚îÄ reports/                   <- An√°lises geradas (HTML, PDF, etc.)
‚îÇ   ‚îî‚îÄ‚îÄ figures/               <- Gr√°ficos e figuras geradas
‚îÇ
‚îî‚îÄ‚îÄ src/                       <- C√≥digo fonte do projeto
    ‚îú‚îÄ‚îÄ __init__.py            <- Torna src um m√≥dulo Python
    ‚îú‚îÄ‚îÄ data_loader.py         <- Download de dados via ML Croissant
    ‚îú‚îÄ‚îÄ preprocessing.py       <- Pipeline de pr√©-processamento e feature engineering
    ‚îú‚îÄ‚îÄ model_training.py      <- Treinamento e avalia√ß√£o de modelos
    ‚îú‚îÄ‚îÄ app.py                 <- Aplica√ß√£o Streamlit para predi√ß√µes
    ‚îú‚îÄ‚îÄ teste_extr√ß√£o_pdf_med.py <- Extra√ß√£o de dados de PDFs m√©dicos
    ‚îî‚îÄ‚îÄ static/                <- Arquivos est√°ticos para o app
        ‚îî‚îÄ‚îÄ style.css          <- Estilos CSS para Streamlit
```

## üõ†Ô∏è Comandos √öteis (Makefile)

> **‚ö†Ô∏è Nota para Usu√°rios Windows**: Os comandos `make` abaixo funcionam apenas em Linux/Mac/WSL. Para Windows, consulte a se√ß√£o "Alternativas para Windows" abaixo.

### Comandos Make (Linux/Mac/WSL)
```bash
# Pipeline completo
make pipeline           # Executa: download ‚Üí preprocess ‚Üí train

# Etapas individuais
make download_data      # Baixa dados do Kaggle via ML Croissant
make preprocess         # Pr√©-processa dados e cria features
make train              # Treina modelo de classifica√ß√£o
make app                # Inicia aplica√ß√£o Streamlit

# Notebooks
make notebooks          # Executa todos os notebooks em sequ√™ncia

# Desenvolvimento
make requirements       # Instala/atualiza depend√™ncias
make clean              # Remove arquivos compilados Python
make lint               # Verifica qualidade do c√≥digo com ruff
make format             # Formata c√≥digo automaticamente com ruff
make create_environment # Cria ambiente conda
make help               # Lista todos os comandos dispon√≠veis
```

### ü™ü Alternativas para Windows (PowerShell)

```powershell
# Ative o ambiente primeiro
conda activate ligia

# Pipeline completo (manual)
python src/data_loader.py
python src/preprocessing.py
python src/model_training.py

# Executar app
streamlit run src/app.py

# Desenvolvimento
python -m pip install -r requirements.txt  # Instalar depend√™ncias

# Limpeza de cache
Get-ChildItem -Recurse -Filter "*.pyc" | Remove-Item -Force
Get-ChildItem -Recurse -Filter "__pycache__" | Remove-Item -Recurse -Force

# Qualidade de c√≥digo
python -m ruff check                       # Verificar c√≥digo
python -m ruff format                      # Formatar c√≥digo
python -m ruff check --fix                 # Corrigir problemas automaticamente
```

## üìù Conven√ß√µes do Projeto

- **Notebooks**: Numera√ß√£o sequencial `X.Y_descricao.ipynb`
  - `1.x` - An√°lise explorat√≥ria
  - `2.x` - Prepara√ß√£o de dados
  - `3.x` - Modelagem
- **Commits**: Seguem [Conventional Commits](https://www.conventionalcommits.org/)
  - `feat:` - Nova funcionalidade
  - `fix:` - Corre√ß√£o de bug
  - `docs:` - Documenta√ß√£o
  - `style:` - Formata√ß√£o
  - `refactor:` - Refatora√ß√£o
- **C√≥digo**: Formatado automaticamente com ruff
- **Dados**: N√£o versionados no Git (`.gitignore`)

--------

